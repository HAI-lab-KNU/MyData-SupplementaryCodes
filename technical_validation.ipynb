{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLVtPgH8rfw0"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9g6zJ19r0gs"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0Z--TR8R_12C"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH_DATA = './Dataset'\n",
    "sensor_data = ['UserInfo.csv', 'Service.csv', 'ContextualFactor.csv', 'Interruptibility.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbFFIwXfG-NX"
   },
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YY8eN6XlYKpq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZh9HiKnHNUE"
   },
   "source": [
    "\n",
    "# Load the Dataset into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oatjHY6UI2fS"
   },
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    filename: pd.read_csv(os.path.join(PATH_DATA, filename)).reset_index(drop=True)\n",
    "    for filename in sensor_data\n",
    "}\n",
    "dfService = dataframes['Service.csv']\n",
    "dfContextualFactor = dataframes['ContextualFactor.csv']\n",
    "dfUserInfo = dataframes['UserInfo.csv']\n",
    "dfInterruptibility = dataframes['Interruptibility.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXvLBvF1tF12"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nSFuapG9u9HX"
   },
   "outputs": [],
   "source": [
    "# Select specific columns from dfContextualFactor, dfService, and dfUserInfo DataFrames\n",
    "# Combine relevant columns to create a unified dataset\n",
    "dfContextualFactor_selected_columns=dfContextualFactor[['uid','activity1','activity2','roomType','userPosition',]]\n",
    "dfService_selected_columns=dfService[['dayOfWeek','startTime']]\n",
    "dfInterruptibility_selected_columns=dfInterruptibility[['SHORT_INTERACTION_interruptibility', 'LONG_INTERACTION_interruptibility']]\n",
    "\n",
    "dfCombinedAll=pd.concat([dfContextualFactor_selected_columns, dfService_selected_columns,dfInterruptibility_selected_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J-7MyCMpNEKf"
   },
   "outputs": [],
   "source": [
    "# Concatenate and merge the all columns from dfUserInfo (dfUserInfo.csv)\n",
    "dfUserInfo_selected_columns = dfUserInfo[['uid', 'homeType', 'speakerRoom', 'speakerPosition']]\n",
    "dfCombinedAll = pd.merge(dfCombinedAll, dfUserInfo_selected_columns, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjFiDMd-zwbv"
   },
   "source": [
    "## Position Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keJCoNLu7m__",
    "outputId": "d45b6516-87c4-41a2-e681-974fd26f7772"
   },
   "outputs": [],
   "source": [
    "# Define function to calculate proximity between user and speaker\n",
    "def calculate_proximity(row):\n",
    "    # Return 0 if user and speaker are in different locations\n",
    "    if row['roomType'] != row['speakerRoom']:\n",
    "        return 0\n",
    "    # If in the same locations\n",
    "    elif row['userPosition'] == row['speakerPosition']:\n",
    "        return 2  # Same position\n",
    "    else:\n",
    "        return 1  # Different positions (including missing position)\n",
    "\n",
    "# Apply proximity calculation to create a new 'proximity' column\n",
    "dfCombinedAll['proximity'] = dfCombinedAll.apply(calculate_proximity, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hentOj4Gzjab"
   },
   "source": [
    "## Activity and Time Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wMot0sto22U8"
   },
   "outputs": [],
   "source": [
    "# Process activity columns for one-hot encoding\n",
    "activity_cols = ['activity1', 'activity2']\n",
    "df_activity = dfContextualFactor[activity_cols].copy()\n",
    "\n",
    "# Get unique activities across all activity columns, excluding NaN\n",
    "all_unique_activities = pd.unique(df_activity.values.ravel())\n",
    "all_unique_activities = [x for x in all_unique_activities if pd.notna(x)]\n",
    "\n",
    "# Create a DataFrame for one-hot encoding of activities\n",
    "dfActivity_one_hot_encoding = pd.DataFrame(0, index=df_activity.index, columns=['act_' + str(val) for val in all_unique_activities])\n",
    "\n",
    "# Perform one-hot encoding for each activity column\n",
    "for col in activity_cols:\n",
    "    for val in all_unique_activities:\n",
    "        dfActivity_one_hot_encoding['act_' + str(val)] |= (df_activity[col] == val).astype(int)\n",
    "\n",
    "# Concatenate one-hot encoded activity columns to dfCombinedAll\n",
    "dfActivity_one_hot_encoding\n",
    "dfCombinedAll = pd.concat([dfCombinedAll, dfActivity_one_hot_encoding], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "bt6nnuMOx00i",
    "outputId": "3faad9bb-8bbd-4417-e91a-516da4ff2251"
   },
   "outputs": [],
   "source": [
    "# Convert startTime to datetime and extract total minutes since midnight\n",
    "dfCombinedAll['startTime'] = pd.to_datetime(dfCombinedAll['startTime'], format='%H:%M:%S', errors='coerce')\n",
    "dfCombinedAll['minute'] = dfCombinedAll['startTime'].dt.hour * 60 + dfCombinedAll['startTime'].dt.minute\n",
    "\n",
    "# Map days of the week to numerical values (MON=0, TUE=1, ..., SUN=6)\n",
    "day_map = {'MON': 0, 'TUE': 1, 'WED': 2, 'THU': 3, 'FRI': 4, 'SAT': 5, 'SUN': 6}\n",
    "dfCombinedAll['dayOfWeek'] = dfCombinedAll['dayOfWeek'].map(day_map)\n",
    "\n",
    "# Bin minutes into 30-minute intervals for temporal analysis\n",
    "dfCombinedAll['startTime_minute'] = (dfCombinedAll['minute'] // 30).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgpGmTtdF7hJ"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "id": "TiDH8e3OI941",
    "outputId": "cc2acd18-c20f-4a8b-b810-b286f7a564a1"
   },
   "outputs": [],
   "source": [
    "# Select features for response prediction\n",
    "dfFeatresForResponse = dfCombinedAll[['uid',\n",
    "    'act_Taking a Nap / Sleeping','act_Hygiene','act_Eating','act_Using Media','act_Social Interaction',\n",
    "    'act_Returning from Outside / Other Rooms','act_Studying / Working','act_Others','act_House Chores',\n",
    "    'act_Self Caring','act_Visiting Outside / Other Rooms','act_Resting',\n",
    "    'homeType','roomType','userPosition','speakerRoom','speakerPosition',\n",
    "    'startTime_minute','dayOfWeek', 'proximity','SHORT_INTERACTION_interruptibility','LONG_INTERACTION_interruptibility']].copy()\n",
    "\n",
    "# Encode categorical columns using LabelEncoder\n",
    "categorical_columns = ['homeType', 'roomType', 'userPosition', 'speakerRoom', 'speakerPosition', 'startTime_minute','SHORT_INTERACTION_interruptibility','LONG_INTERACTION_interruptibility']\n",
    "label_encoders = defaultdict(LabelEncoder)\n",
    "\n",
    "# Apply label encoding to each categorical column\n",
    "for col in categorical_columns:\n",
    "    dfFeatresForResponse[col] = label_encoders[col].fit_transform(dfFeatresForResponse[col])\n",
    "\n",
    "# Create a copy of the encoded data for further processing\n",
    "encoded_data = dfFeatresForResponse.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMRZiUdWthAG"
   },
   "source": [
    "## Label: SHORT_INTERACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7R3RWMV4n8-",
    "outputId": "77f86bd9-b1fc-48ae-fb86-fc413af67e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHORT_INTERACTION_interruptibility\n",
      "0    2088\n",
      "1     742\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Label distribution before balancing\n",
    "print(encoded_data['SHORT_INTERACTION_interruptibility'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsOz6lgfpGjB"
   },
   "source": [
    "### Model Building and LOSO CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2830 entries, 0 to 2829\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                    Non-Null Count  Dtype\n",
      "---  ------                                    --------------  -----\n",
      " 0   act_Taking a Nap / Sleeping               2830 non-null   int64\n",
      " 1   act_Hygiene                               2830 non-null   int64\n",
      " 2   act_Eating                                2830 non-null   int64\n",
      " 3   act_Using Media                           2830 non-null   int64\n",
      " 4   act_Social Interaction                    2830 non-null   int64\n",
      " 5   act_Returning from Outside / Other Rooms  2830 non-null   int64\n",
      " 6   act_Studying / Working                    2830 non-null   int64\n",
      " 7   act_Others                                2830 non-null   int64\n",
      " 8   act_House Chores                          2830 non-null   int64\n",
      " 9   act_Self Caring                           2830 non-null   int64\n",
      " 10  act_Visiting Outside / Other Rooms        2830 non-null   int64\n",
      " 11  act_Resting                               2830 non-null   int64\n",
      " 12  homeType                                  2830 non-null   int64\n",
      " 13  roomType                                  2830 non-null   int64\n",
      " 14  userPosition                              2830 non-null   int64\n",
      " 15  speakerRoom                               2830 non-null   int64\n",
      " 16  speakerPosition                           2830 non-null   int64\n",
      " 17  startTime_minute                          2830 non-null   int64\n",
      " 18  dayOfWeek                                 2830 non-null   int64\n",
      " 19  proximity                                 2830 non-null   int64\n",
      "dtypes: int64(20)\n",
      "memory usage: 442.3 KB\n",
      "None\n",
      "\n",
      "Label (y):\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 2830 entries, 0 to 2829\n",
      "Series name: SHORT_INTERACTION_interruptibility\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "2830 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 22.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prepare features (X) and target variables (y) for SHORT_INTERACTION\n",
    "X = encoded_data.drop(columns=['SHORT_INTERACTION_interruptibility', 'LONG_INTERACTION_interruptibility', 'uid'], axis=1) # Drop target and unrelated columns\n",
    "y = encoded_data['SHORT_INTERACTION_interruptibility'] # Target variable\n",
    "groups = encoded_data['uid'] # Group by user ID for Leave-One-Group-Out CV\n",
    "\n",
    "print(\"Features (X):\")\n",
    "print(X.info())\n",
    "print(\"\\nLabel (y):\")\n",
    "print(y.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrKX-wJlMMp9",
    "outputId": "3d573ad8-f06e-4f47-f442-9636a9a7325e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Leave-One-Group-Out cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Initialize SMOTE for oversampling to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Define models to evaluate\n",
    "models1 = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, verbose=0),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, verbosity=0, use_label_encoder=False),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(iterations=100, depth=5, learning_rate=0.1, loss_function='Logloss', cat_features=[0], random_seed=42, verbose=0),\n",
    "    'SVM': SVC(random_state=42, verbose=False),\n",
    "    'Dummy': DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "}\n",
    "\n",
    "results1_logo = {}\n",
    "\n",
    "# Loop over each model\n",
    "for model_name, model1 in models1.items():\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Loop over each fold in Leave-One-Group-Out cross-validation\n",
    "    for i, (train_index, test_index) in enumerate(logo.split(X, y, groups)):\n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Apply SMOTE to the training data to balance class distribution\n",
    "        X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Train the model on the oversampled training data\n",
    "        model1.fit(X_train_oversampled, y_train_oversampled.to_numpy())\n",
    "\n",
    "        # Predict the target on the test data\n",
    "        y_pred = model1.predict(X_test)\n",
    "\n",
    "        # Evaluate the prediction\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "    # Compute average accuracy and F1-score across all folds\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    results1_logo[model_name] = {\n",
    "        'Average Accuracy': avg_accuracy,\n",
    "        'Average F1-Score (macro)': avg_f1_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YD9tgjzEppKh"
   },
   "source": [
    "### K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ITwfO-xNYwH",
    "outputId": "3a817c09-01fb-4962-abe4-1c186db6e9ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize 5-fold cross-validator\n",
    "kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "results1_kfold = {}\n",
    "\n",
    "# Loop over each model\n",
    "for model_name, model1 in models1.items():\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Loop over each fold in 5-fold cross-validation\n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Apply SMOTE to the training data to balance class distribution\n",
    "        X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Train the model on the oversampled training data\n",
    "        model1.fit(X_train_oversampled, y_train_oversampled.to_numpy())\n",
    "\n",
    "        # Predict the target on the test data\n",
    "        y_pred = model1.predict(X_test)\n",
    "\n",
    "        # Evaluate the prediction\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Compute average accuracy and F1-score across all folds\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    results1_kfold[model_name] = {\n",
    "        'Average Accuracy': avg_accuracy,\n",
    "        'Average F1-Score (macro)': avg_f1_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCsjDX20tfim"
   },
   "source": [
    "## Label: LONG_INTERACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZVe51NM4i6P",
    "outputId": "508395cd-a696-4176-db4c-33caa2236916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG_INTERACTION_interruptibility\n",
      "1    1443\n",
      "0    1387\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Label distribution before balancing\n",
    "print(encoded_data['LONG_INTERACTION_interruptibility'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SkfxAFQqEwa"
   },
   "source": [
    "### Model building and LOSO CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2830 entries, 0 to 2829\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                    Non-Null Count  Dtype\n",
      "---  ------                                    --------------  -----\n",
      " 0   act_Taking a Nap / Sleeping               2830 non-null   int64\n",
      " 1   act_Hygiene                               2830 non-null   int64\n",
      " 2   act_Eating                                2830 non-null   int64\n",
      " 3   act_Using Media                           2830 non-null   int64\n",
      " 4   act_Social Interaction                    2830 non-null   int64\n",
      " 5   act_Returning from Outside / Other Rooms  2830 non-null   int64\n",
      " 6   act_Studying / Working                    2830 non-null   int64\n",
      " 7   act_Others                                2830 non-null   int64\n",
      " 8   act_House Chores                          2830 non-null   int64\n",
      " 9   act_Self Caring                           2830 non-null   int64\n",
      " 10  act_Visiting Outside / Other Rooms        2830 non-null   int64\n",
      " 11  act_Resting                               2830 non-null   int64\n",
      " 12  homeType                                  2830 non-null   int64\n",
      " 13  roomType                                  2830 non-null   int64\n",
      " 14  userPosition                              2830 non-null   int64\n",
      " 15  speakerRoom                               2830 non-null   int64\n",
      " 16  speakerPosition                           2830 non-null   int64\n",
      " 17  startTime_minute                          2830 non-null   int64\n",
      " 18  dayOfWeek                                 2830 non-null   int64\n",
      " 19  proximity                                 2830 non-null   int64\n",
      "dtypes: int64(20)\n",
      "memory usage: 442.3 KB\n",
      "None\n",
      "\n",
      "Label (y):\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 2830 entries, 0 to 2829\n",
      "Series name: LONG_INTERACTION_interruptibility\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "2830 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 22.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prepare features (X) and target variable (y) for predicting LONG_INTERACTION\n",
    "X = encoded_data.drop(columns=['LONG_INTERACTION_interruptibility', 'SHORT_INTERACTION_interruptibility','uid'], axis=1)\n",
    "y = encoded_data['LONG_INTERACTION_interruptibility']\n",
    "groups = encoded_data['uid']\n",
    "\n",
    "print(\"Features (X):\")\n",
    "print(X.info())\n",
    "print(\"\\nLabel (y):\")\n",
    "print(y.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSZBAAdESGEd",
    "outputId": "390ec19e-29d5-4b39-eaa5-7ca6293420bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Leave-One-Group-Out cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Initialize SMOTE for oversampling to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Define models to evaluate\n",
    "models2 = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, verbose=0),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, verbosity=0, use_label_encoder=False),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(iterations=100, depth=5, learning_rate=0.1, loss_function='Logloss', cat_features=[0], random_seed=42, verbose=0),\n",
    "    'SVM': SVC(random_state=42, verbose=False),\n",
    "    'Dummy': DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "}\n",
    "\n",
    "results2_logo = {}\n",
    "\n",
    "# Loop over each model\n",
    "for model_name, model2 in models2.items():\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Loop over each fold in Leave-One-Group-Out cross-validation\n",
    "    for i, (train_index, test_index) in enumerate(logo.split(X, y, groups)):\n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Apply SMOTE to the training data to balance class distribution\n",
    "        X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Train the model on the oversampled training data\n",
    "        model2.fit(X_train_oversampled, y_train_oversampled.to_numpy())\n",
    "\n",
    "        # Predict the target on the test data\n",
    "        y_pred = model2.predict(X_test)\n",
    "\n",
    "        # Evaluate the prediction\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Compute average accuracy and F1-score across all folds\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    results2_logo[model_name] = {\n",
    "        'Average Accuracy': avg_accuracy,\n",
    "        'Average F1-Score (macro)': avg_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3smYpTiAqqQh"
   },
   "source": [
    "### K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ah_QrD67SnHN",
    "outputId": "8cf8cc9f-d0c2-4934-e730-7874ecb6cb97",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize 5-fold cross-validator\n",
    "kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "results2_kfold = {}\n",
    "\n",
    "# Loop over each model\n",
    "for model_name, model2 in models2.items():\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Loop over each fold in 5-fold cross-validation\n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        # Split the data into training and testing sets for the current fold\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Apply SMOTE to the training data to balance class distribution\n",
    "        X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Train the model on the oversampled training data\n",
    "        model2.fit(X_train_oversampled, y_train_oversampled.to_numpy())\n",
    "\n",
    "        # Predict the target on the test data\n",
    "        y_pred = model2.predict(X_test)\n",
    "\n",
    "        # Evaluate the prediction\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Compute average accuracy and F1-score across all folds\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    results2_kfold[model_name] = {\n",
    "        'Average Accuracy': avg_accuracy,\n",
    "        'Average F1-Score (macro)': avg_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Visualization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning model performance\n",
      "+-------------------+------------+-----------+------------+-----------+-------------+-------------+-------------+-------------+\n",
      "| Model             |    LOSO CV |   LOSO CV |    LOSO CV |   LOSO CV |   5-fold CV |   5-fold CV |   5-fold CV |   5-fold CV |\n",
      "|                   |      Short |     Short |       Long |      Long |       Short |       Short |        Long |        Long |\n",
      "|                   |    Interac |   Interac |    Interac |   Interac |     Interac |     Interac |     Interac |     Interac |\n",
      "|                   |   Accuracy |        F1 |   Accuracy |        F1 |    Accuracy |          F1 |    Accuracy |          F1 |\n",
      "+===================+============+===========+============+===========+=============+=============+=============+=============+\n",
      "| Random Forest     |      0.810 |     0.722 |      0.664 |     0.619 |       0.807 |       0.765 |       0.659 |       0.656 |\n",
      "+-------------------+------------+-----------+------------+-----------+-------------+-------------+-------------+-------------+\n",
      "| Gradient Boosting |      0.808 |     0.715 |      0.675 |     0.613 |       0.802 |       0.750 |       0.662 |       0.657 |\n",
      "+-------------------+------------+-----------+------------+-----------+-------------+-------------+-------------+-------------+\n",
      "| XGBoost           |      0.807 |     0.711 |      0.675 |     0.612 |       0.803 |       0.760 |       0.672 |       0.667 |\n",
      "+-------------------+------------+-----------+------------+-----------+-------------+-------------+-------------+-------------+\n",
      "| LightGBM          |      0.798 |     0.706 |      0.683 |     0.619 |       0.796 |       0.753 |       0.673 |       0.668 |\n",
      "+-------------------+------------+-----------+------------+-----------+-------------+-------------+-------------+-------------+\n",
      "| CatBoost          |      0.807 |     0.716 |      0.692 |     0.630 |       0.806 |       0.762 |       0.680 |       0.676 |\n",
      "+-------------------+------------+-----------+------------+-----------+-------------+-------------+-------------+-------------+\n",
      "| SVM               |      0.790 |     0.703 |      0.672 |     0.622 |       0.777 |       0.747 |       0.637 |       0.636 |\n",
      "+-------------------+------------+-----------+------------+-----------+-------------+-------------+-------------+-------------+\n",
      "| Dummy             |      0.493 |     0.448 |      0.512 |     0.479 |       0.515 |       0.476 |       0.504 |       0.503 |\n",
      "+-------------------+------------+-----------+------------+-----------+-------------+-------------+-------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Models list\n",
    "models = [\n",
    "    'Random Forest', 'Gradient Boosting', 'XGBoost',\n",
    "    'LightGBM', 'CatBoost', 'SVM', 'Dummy'\n",
    "]\n",
    "\n",
    "# Prepare table data\n",
    "table_data = []\n",
    "for model in models:\n",
    "    row = [model]\n",
    "    \n",
    "    # Shell 1: SHORT_INTERACTION, Leave-One-Group-Out\n",
    "    row.append(results1_logo.get(model, {}).get('Average Accuracy', 0.0))\n",
    "    row.append(results1_logo.get(model, {}).get('Average F1-Score (macro)', 0.0))\n",
    "    \n",
    "    # Shell 3: LONG_INTERACTION, Leave-One-Group-Out\n",
    "    row.append(results2_logo.get(model, {}).get('Average Accuracy', 0.0))\n",
    "    row.append(results2_logo.get(model, {}).get('Average F1-Score (macro)', 0.0))\n",
    "\n",
    "    # Shell 2: SHORT_INTERACTION, 5-fold\n",
    "    row.append(results1_kfold.get(model, {}).get('Average Accuracy', 0.0))\n",
    "    row.append(results1_kfold.get(model, {}).get('Average F1-Score (macro)', 0.0))\n",
    "    \n",
    "    # Shell 4: LONG_INTERACTION, 5-fold\n",
    "    row.append(results2_kfold.get(model, {}).get('Average Accuracy', 0.0))\n",
    "    row.append(results2_kfold.get(model, {}).get('Average F1-Score (macro)', 0.0))\n",
    "    \n",
    "    table_data.append(row)\n",
    "\n",
    "# Define headers\n",
    "headers = [\n",
    "    'Model',\n",
    "    'LOSO CV\\nShort\\nInterac\\nAccuracy', 'LOSO CV\\nShort\\nInterac\\nF1',\n",
    "    'LOSO CV\\nLong\\nInterac\\nAccuracy', 'LOSO CV\\nLong\\nInterac\\nF1',\n",
    "    '5-fold CV\\nShort\\nInterac\\nAccuracy', '5-fold CV\\nShort\\nInterac\\nF1',\n",
    "    '5-fold CV\\nLong\\nInterac\\nAccuracy', '5-fold CV\\nLong\\nInterac\\nF1'\n",
    "]\n",
    "\n",
    "# Print the table\n",
    "print(\"Machine learning model performance\")\n",
    "print(tabulate(table_data, headers=headers, tablefmt='grid', floatfmt='.3f', numalign=\"decimal\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
